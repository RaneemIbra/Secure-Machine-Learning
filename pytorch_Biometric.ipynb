{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\Alpha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Alpha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Alpha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Alpha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Alpha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Alpha\\AppData\\Local\\Temp\\ipykernel_13352\\2953782152.py\", line 4, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\__init__.py\", line 643, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\functional.py\", line 6, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 2, in <module>\n",
      "    from .linear import Identity, Linear, Bilinear, LazyLinear\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 6, in <module>\n",
      "    from .. import functional as F\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py\", line 11, in <module>\n",
      "    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\_jit_internal.py\", line 28, in <module>\n",
      "    import torch.package._mangling as package_mangling\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\package\\__init__.py\", line 12, in <module>\n",
      "    from .package_importer import PackageImporter\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\package\\package_importer.py\", line 16, in <module>\n",
      "    from ._directory_reader import DirectoryReader\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\package\\_directory_reader.py\", line 17, in <module>\n",
      "    _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "  File \"c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\package\\_directory_reader.py\", line 17, in <dictcomp>\n",
      "    _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\torch\\package\\_directory_reader.py:17: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Alpha\\Desktop\\Project_in_secure_ML\\pytorch_env\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ONNXRuntime providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Using device: cuda\n",
      "Train pairs: 2200, Test pairs: 1000\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (112, 112)\n",
      "Train pairs (split): 2560\n",
      "Test pairs (split): 640\n",
      "Building ArcFace embeddings for train set...\n",
      "Building ArcFace embeddings for test set...\n",
      "\n",
      "=== Training with Full Precision (no truncation) ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 314\u001b[0m\n\u001b[0;32m    312\u001b[0m pair_mlp_full \u001b[38;5;241m=\u001b[39m PairMLP(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, hidden_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m], dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m    313\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# For demonstration, reduce or increase as needed\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m hist_full \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mlp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpair_mlp_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\n\u001b[0;32m    323\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Plot training curves for full precision\u001b[39;00m\n\u001b[0;32m    326\u001b[0m epochs_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 237\u001b[0m, in \u001b[0;36mtrain_mlp\u001b[1;34m(pair_mlp, train_dataset, test_dataset, epochs, batch_size, lr, alpha, gamma)\u001b[0m\n\u001b[0;32m    234\u001b[0m     pred_bin \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    235\u001b[0m     correct_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred_bin \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m--> 237\u001b[0m     y_true_train\u001b[38;5;241m.\u001b[39mextend(\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    238\u001b[0m     y_pred_train\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m    240\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m total_train\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Install insightface if not installed (>= 0.7+):\n",
    "# pip install insightface opencv-python\n",
    "\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "import onnxruntime\n",
    "print(onnxruntime.get_available_providers())\n",
    "\n",
    "# --------------------------------------------------\n",
    "#               DEVICE + CLEAR CACHE\n",
    "# --------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "#            DATASET & PATHS\n",
    "# --------------------------------------------------\n",
    "base_dir = os.path.join('dataset', 'lfw-deepfunneled', 'lfw-deepfunneled')\n",
    "pairs_train_path = 'pairsDevTrain.txt'\n",
    "pairs_test_path = 'pairsDevTest.txt'\n",
    "\n",
    "def normalize_name(name):\n",
    "    return name.replace(' ', '_')\n",
    "\n",
    "def load_pairs(pairs_path, base_dir):\n",
    "    \"\"\"\n",
    "    Parse the LFW pairs file and return a list of (img1_path, img2_path, label).\n",
    "    label=1 => same person, label=0 => different.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    with open(pairs_path, 'r') as f:\n",
    "        lines = f.readlines()[1:]  # skip header\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:  # Positive pair\n",
    "                person, img1, img2 = parts\n",
    "                person = normalize_name(person)\n",
    "                img1_path = os.path.join(base_dir, person, f\"{person}_{int(img1):04d}.jpg\")\n",
    "                img2_path = os.path.join(base_dir, person, f\"{person}_{int(img2):04d}.jpg\")\n",
    "                if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                    pairs.append((img1_path, img2_path, 1))\n",
    "            elif len(parts) == 4:  # Negative pair\n",
    "                p1, img1, p2, img2 = parts\n",
    "                p1, p2 = normalize_name(p1), normalize_name(p2)\n",
    "                img1_path = os.path.join(base_dir, p1, f\"{p1}_{int(img1):04d}.jpg\")\n",
    "                img2_path = os.path.join(base_dir, p2, f\"{p2}_{int(img2):04d}.jpg\")\n",
    "                if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                    pairs.append((img1_path, img2_path, 0))\n",
    "    return pairs\n",
    "\n",
    "# Load LFW pairs\n",
    "train_pairs = load_pairs(pairs_train_path, base_dir)\n",
    "test_pairs = load_pairs(pairs_test_path, base_dir)\n",
    "all_pairs = train_pairs + test_pairs\n",
    "\n",
    "# --------------------------------------------------\n",
    "#   ARCFACE (R100) VIA INSIGHTFACE \"buffalo_l\"\n",
    "# --------------------------------------------------\n",
    "app = FaceAnalysis(\n",
    "    name='buffalo_l', \n",
    "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    ")\n",
    "app.prepare(ctx_id=0, det_size=(112,112))  # Assume 112x112\n",
    "\n",
    "def get_arcface_embedding(img_path):\n",
    "    bgr_img = cv2.imread(img_path)\n",
    "    if bgr_img is None:\n",
    "        raise ValueError(f\"Could not load image: {img_path}\")\n",
    "    bgr_img = cv2.resize(bgr_img, (112, 112))\n",
    "    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "    recognition_model = app.models['recognition']  # ArcFaceONNX\n",
    "    feat = recognition_model.get_feat(rgb_img)\n",
    "    feat_norm = feat / np.linalg.norm(feat)\n",
    "    return feat_norm.astype(np.float32)\n",
    "\n",
    "# --------------------------------------------------\n",
    "#   GENERATE & CACHE EMBEDDINGS\n",
    "# --------------------------------------------------\n",
    "def build_embeddings_dict(pairs_list):\n",
    "    unique_paths = set()\n",
    "    for (img1, img2, _) in pairs_list:\n",
    "        unique_paths.add(img1)\n",
    "        unique_paths.add(img2)\n",
    "    unique_paths = list(unique_paths)\n",
    "\n",
    "    emb_dict = {}\n",
    "    for path in unique_paths:\n",
    "        emb = get_arcface_embedding(path)  # shape=(512,)\n",
    "        emb_dict[path] = emb\n",
    "    return emb_dict\n",
    "\n",
    "# --------------------------------------------------\n",
    "#   MLP CLASSIFIER ON abs-diff OF EMBEDDINGS\n",
    "# --------------------------------------------------\n",
    "class PairMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes input shape=(N,512) => outputs prob of \"same person\".\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=512, hidden_dims=[256, 128], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, 512)\n",
    "        return torch.sigmoid(self.net(x))\n",
    "\n",
    "# --------------------------------------------------\n",
    "#       FOCAL LOSS (Optional)\n",
    "# --------------------------------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCELoss(reduction='none')\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = self.bce(preds, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal.mean()\n",
    "\n",
    "# --------------------------------------------------\n",
    "#   DATASET FOR (abs-diff, label)\n",
    "# --------------------------------------------------\n",
    "class DiffLabelDataset(Dataset):\n",
    "    def __init__(self, pairs_list, emb_dict, decimals=4):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        self.decimals = decimals\n",
    "        scale = 10**decimals if decimals else None\n",
    "\n",
    "        for (img1, img2, lab) in pairs_list:\n",
    "            emb1 = emb_dict[img1].copy()\n",
    "            emb2 = emb_dict[img2].copy()\n",
    "            if scale:\n",
    "                emb1 = np.floor(emb1 * scale) / scale\n",
    "                emb2 = np.floor(emb2 * scale) / scale\n",
    "            diff = np.abs(emb1 - emb2).astype(np.float32)\n",
    "            self.samples.append((diff, float(lab)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        diff, lab = self.samples[idx]\n",
    "        return torch.tensor(diff, dtype=torch.float32), torch.tensor(lab, dtype=torch.float32)\n",
    "\n",
    "# --------------------------------------------------\n",
    "#        TRAIN MLP ON (diff,label)\n",
    "# --------------------------------------------------\n",
    "def train_mlp(\n",
    "    pair_mlp,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    alpha=0.25,\n",
    "    gamma=2.0\n",
    "):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    pair_mlp.to(device)\n",
    "    optimizer = optim.Adam(pair_mlp.parameters(), lr=lr)\n",
    "    criterion = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'test_loss': [],\n",
    "        'train_acc': [], 'test_acc': [],\n",
    "        'train_auc': [], 'test_auc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---------- TRAIN ----------\n",
    "        pair_mlp.train()\n",
    "        total_train, correct_train = 0, 0\n",
    "        running_loss = 0.0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "\n",
    "        for diffs, labels in train_loader:\n",
    "            diffs, labels = diffs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = pair_mlp(diffs)\n",
    "            loss = criterion(preds, labels.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "            pred_bin = (preds > 0.5).float().squeeze()\n",
    "            correct_train += (pred_bin == labels).sum().item()\n",
    "\n",
    "            y_true_train.extend(labels.cpu().numpy())\n",
    "            y_pred_train.extend(preds.detach().cpu().numpy().flatten())\n",
    "\n",
    "        epoch_loss = running_loss / total_train\n",
    "        epoch_acc = correct_train / total_train\n",
    "        epoch_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        history['train_auc'].append(epoch_auc)\n",
    "\n",
    "        # ---------- EVAL ----------\n",
    "        pair_mlp.eval()\n",
    "        total_test, correct_test = 0, 0\n",
    "        test_loss = 0.0\n",
    "        y_true_test, y_pred_test = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for diffs, labels in test_loader:\n",
    "                diffs, labels = diffs.to(device), labels.to(device)\n",
    "                preds = pair_mlp(diffs)\n",
    "                loss_val = criterion(preds, labels.unsqueeze(1))\n",
    "\n",
    "                test_loss += loss_val.item() * labels.size(0)\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "                pred_bin = (preds > 0.5).float().squeeze()\n",
    "                correct_test += (pred_bin == labels).sum().item()\n",
    "\n",
    "                y_true_test.extend(labels.cpu().numpy())\n",
    "                y_pred_test.extend(preds.cpu().numpy().flatten())\n",
    "\n",
    "        epoch_test_loss = test_loss / total_test\n",
    "        epoch_test_acc = correct_test / total_test\n",
    "        epoch_test_auc = roc_auc_score(y_true_test, y_pred_test)\n",
    "\n",
    "        history['test_loss'].append(epoch_test_loss)\n",
    "        history['test_acc'].append(epoch_test_acc)\n",
    "        history['test_auc'].append(epoch_test_auc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss={epoch_loss:.4f}, Acc={epoch_acc*100:.2f}%, AUC={epoch_auc:.4f} | \"\n",
    "              f\"Test Loss={epoch_test_loss:.4f}, Acc={epoch_test_acc*100:.2f}%, AUC={epoch_test_auc:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# --------------------------------------------------\n",
    "#                 MAIN\n",
    "# --------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    X_train_pairs, X_test_pairs = train_test_split(\n",
    "        all_pairs,\n",
    "        test_size=0.2,\n",
    "        stratify=[p[2] for p in all_pairs],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Building ArcFace embeddings for train set...\")\n",
    "    emb_dict_train = build_embeddings_dict(X_train_pairs)\n",
    "    print(\"Building ArcFace embeddings for test set...\")\n",
    "    emb_dict_test = build_embeddings_dict(X_test_pairs)\n",
    "\n",
    "    DECIMALS = 4\n",
    "    train_dataset = DiffLabelDataset(X_train_pairs, emb_dict_train, decimals=DECIMALS)\n",
    "    test_dataset = DiffLabelDataset(X_test_pairs, emb_dict_test, decimals=DECIMALS)\n",
    "\n",
    "    pair_mlp = PairMLP(input_dim=512, hidden_dims=[256,128], dropout=0.3)\n",
    "\n",
    "    EPOCHS = 10\n",
    "    hist = train_mlp(\n",
    "        pair_mlp,\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=64,\n",
    "        lr=1e-3,\n",
    "        alpha=0.25,\n",
    "        gamma=2.0\n",
    "    )\n",
    "\n",
    "    epochs_axis = range(1, EPOCHS+1)\n",
    "    plt.figure(figsize=(18,5))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(epochs_axis, hist['train_loss'], label='Train Loss')\n",
    "    plt.plot(epochs_axis, hist['test_loss'], label='Test Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.grid(); plt.legend()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(epochs_axis, hist['train_acc'], label='Train Acc')\n",
    "    plt.plot(epochs_axis, hist['test_acc'], label='Test Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.grid(); plt.legend()    \n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(epochs_axis, hist['train_auc'], label='Train AUC')\n",
    "    plt.plot(epochs_axis, hist['test_auc'], label='Test AUC')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('AUC')\n",
    "    plt.title('AUC vs. Epochs')\n",
    "    plt.grid(); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
