{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13233 images from 5749 classes.\n",
      "Epoch 1/30\n",
      "331/331 [==============================] - 155s 333ms/step - loss: 10.9013 - accuracy: 0.0447 - val_loss: 9.9721 - val_accuracy: 0.0601 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "331/331 [==============================] - 111s 334ms/step - loss: 9.2991 - accuracy: 0.0678 - val_loss: 9.0118 - val_accuracy: 0.0778 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "331/331 [==============================] - 109s 328ms/step - loss: 8.5292 - accuracy: 0.0799 - val_loss: 8.4919 - val_accuracy: 0.0869 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "331/331 [==============================] - 112s 337ms/step - loss: 8.0000 - accuracy: 0.0889 - val_loss: 8.2909 - val_accuracy: 0.0997 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "331/331 [==============================] - 112s 336ms/step - loss: 7.6005 - accuracy: 0.0989 - val_loss: 8.2338 - val_accuracy: 0.0982 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "331/331 [==============================] - 111s 333ms/step - loss: 7.3277 - accuracy: 0.1052 - val_loss: 8.0227 - val_accuracy: 0.1152 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "331/331 [==============================] - 113s 341ms/step - loss: 7.0701 - accuracy: 0.1119 - val_loss: 7.9663 - val_accuracy: 0.1360 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "331/331 [==============================] - 112s 337ms/step - loss: 6.8789 - accuracy: 0.1228 - val_loss: 7.7593 - val_accuracy: 0.1454 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "331/331 [==============================] - 113s 342ms/step - loss: 6.6666 - accuracy: 0.1305 - val_loss: 8.5344 - val_accuracy: 0.1356 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "331/331 [==============================] - 112s 337ms/step - loss: 6.4995 - accuracy: 0.1448 - val_loss: 8.0082 - val_accuracy: 0.1598 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "331/331 [==============================] - ETA: 0s - loss: 6.3550 - accuracy: 0.1494\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "331/331 [==============================] - 112s 337ms/step - loss: 6.3550 - accuracy: 0.1494 - val_loss: 8.1015 - val_accuracy: 0.1504 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "331/331 [==============================] - 110s 332ms/step - loss: 6.1218 - accuracy: 0.1615 - val_loss: 8.0843 - val_accuracy: 0.1776 - lr: 5.0000e-05\n",
      "Epoch 13/30\n",
      "331/331 [==============================] - 115s 348ms/step - loss: 6.0002 - accuracy: 0.1704 - val_loss: 7.6840 - val_accuracy: 0.1904 - lr: 5.0000e-05\n",
      "Epoch 14/30\n",
      "331/331 [==============================] - 109s 330ms/step - loss: 5.9006 - accuracy: 0.1749 - val_loss: 7.9102 - val_accuracy: 0.1961 - lr: 5.0000e-05\n",
      "Epoch 15/30\n",
      "331/331 [==============================] - 108s 326ms/step - loss: 5.8124 - accuracy: 0.1806 - val_loss: 8.0427 - val_accuracy: 0.1972 - lr: 5.0000e-05\n",
      "Epoch 16/30\n",
      "331/331 [==============================] - ETA: 0s - loss: 5.7240 - accuracy: 0.1899\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "331/331 [==============================] - 109s 329ms/step - loss: 5.7240 - accuracy: 0.1899 - val_loss: 8.1109 - val_accuracy: 0.2059 - lr: 5.0000e-05\n",
      "Epoch 17/30\n",
      "331/331 [==============================] - 110s 332ms/step - loss: 5.5945 - accuracy: 0.1976 - val_loss: 7.9284 - val_accuracy: 0.2157 - lr: 2.5000e-05\n",
      "Epoch 18/30\n",
      "331/331 [==============================] - 113s 340ms/step - loss: 5.5150 - accuracy: 0.2027 - val_loss: 7.9676 - val_accuracy: 0.2138 - lr: 2.5000e-05\n",
      "Epoch 19/30\n",
      "331/331 [==============================] - ETA: 0s - loss: 5.4724 - accuracy: 0.2037\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "331/331 [==============================] - 116s 349ms/step - loss: 5.4724 - accuracy: 0.2037 - val_loss: 7.7655 - val_accuracy: 0.2199 - lr: 2.5000e-05\n",
      "Epoch 20/30\n",
      "331/331 [==============================] - 109s 329ms/step - loss: 5.3826 - accuracy: 0.2130 - val_loss: 7.9618 - val_accuracy: 0.2274 - lr: 1.2500e-05\n",
      "Epoch 21/30\n",
      "331/331 [==============================] - 106s 320ms/step - loss: 5.3267 - accuracy: 0.2176 - val_loss: 7.8386 - val_accuracy: 0.2293 - lr: 1.2500e-05\n",
      "Epoch 22/30\n",
      "331/331 [==============================] - ETA: 0s - loss: 5.3019 - accuracy: 0.2210\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "331/331 [==============================] - 107s 321ms/step - loss: 5.3019 - accuracy: 0.2210 - val_loss: 8.0025 - val_accuracy: 0.2304 - lr: 1.2500e-05\n",
      "Epoch 23/30\n",
      "331/331 [==============================] - 108s 325ms/step - loss: 5.2507 - accuracy: 0.2248 - val_loss: 8.0502 - val_accuracy: 0.2289 - lr: 6.2500e-06\n",
      "Epoch 24/30\n",
      "331/331 [==============================] - 107s 321ms/step - loss: 5.2402 - accuracy: 0.2267 - val_loss: 8.1554 - val_accuracy: 0.2350 - lr: 6.2500e-06\n",
      "Epoch 25/30\n",
      "331/331 [==============================] - ETA: 0s - loss: 5.2180 - accuracy: 0.2231\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "331/331 [==============================] - 113s 341ms/step - loss: 5.2180 - accuracy: 0.2231 - val_loss: 7.9750 - val_accuracy: 0.2354 - lr: 6.2500e-06\n",
      "Epoch 26/30\n",
      "331/331 [==============================] - 116s 350ms/step - loss: 5.1998 - accuracy: 0.2280 - val_loss: 8.0814 - val_accuracy: 0.2365 - lr: 3.1250e-06\n",
      "Epoch 27/30\n",
      "331/331 [==============================] - 115s 347ms/step - loss: 5.1916 - accuracy: 0.2267 - val_loss: 8.0457 - val_accuracy: 0.2391 - lr: 3.1250e-06\n",
      "Epoch 28/30\n",
      "331/331 [==============================] - ETA: 0s - loss: 5.1736 - accuracy: 0.2283\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "331/331 [==============================] - 118s 354ms/step - loss: 5.1736 - accuracy: 0.2283 - val_loss: 8.1942 - val_accuracy: 0.2410 - lr: 3.1250e-06\n",
      "Epoch 29/30\n",
      "331/331 [==============================] - 113s 340ms/step - loss: 5.1759 - accuracy: 0.2286 - val_loss: 8.1317 - val_accuracy: 0.2406 - lr: 1.5625e-06\n",
      "Epoch 30/30\n",
      "331/331 [==============================] - 113s 340ms/step - loss: 5.1571 - accuracy: 0.2313 - val_loss: 8.1251 - val_accuracy: 0.2418 - lr: 1.5625e-06\n",
      "83/83 [==============================] - 7s 73ms/step\n",
      "Classification Accuracy: 24.18%\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Extract embeddings for biometric identification\u001b[39;00m\n\u001b[0;32m    111\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mbase_model\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mbase_model\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m--> 112\u001b[0m embeddings_test \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m embeddings_train \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity for biometric identification\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alpha\\Desktop\\Project in secure ML\\venv_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Alpha\\Desktop\\Project in secure ML\\venv_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def load_dataset(data_dir, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    for idx, person_name in enumerate(os.listdir(data_dir)):\n",
    "        person_dir = os.path.join(data_dir, person_name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "        class_names.append(person_name)\n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):  # Skip non-image files\n",
    "                continue\n",
    "            try:\n",
    "                img = load_img(img_path, target_size=target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                images.append(preprocess_input(img_array, version=2))\n",
    "                labels.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels), class_names\n",
    "\n",
    "# Path to the dataset\n",
    "data_dir = r'C:\\Users\\Alpha\\Desktop\\Project in secure ML\\dataset\\lfw-deepfunneled\\lfw-deepfunneled'\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "images, labels, class_names = load_dataset(data_dir)\n",
    "print(f\"Loaded {len(images)} images from {len(class_names)} classes.\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(class_names))\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(class_names))\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Load the VGGFace model as a base model\n",
    "base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "\n",
    "# Unfreeze the last few layers of the base model for fine-tuning\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Define the complete model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(512, activation='relu'),  # Added an additional dense layer\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # Added L2 regularization\n",
    "    Dropout(0.5),\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,  # Increased number of epochs\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f'Classification Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Extract embeddings for biometric identification\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "embeddings_test = feature_extractor.predict(X_test)\n",
    "embeddings_train = feature_extractor.predict(X_train)\n",
    "\n",
    "# Compute cosine similarity for biometric identification\n",
    "scores = cosine_similarity(embeddings_test, embeddings_train)\n",
    "\n",
    "# Identify closest matches\n",
    "closest_match = np.argmax(scores, axis=1)\n",
    "biometric_accuracy = np.mean(closest_match == np.argmax(y_test, axis=1))\n",
    "print(f'Biometric Identification Accuracy: {biometric_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
