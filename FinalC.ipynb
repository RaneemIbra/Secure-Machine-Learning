{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation for Homomorphic Similarity Matrix Computation\n",
    "\n",
    "## Overview\n",
    "This script demonstrates the computation of a similarity matrix between face image embeddings using **cleartext operations** and **homomorphic encryption** (HE). The embeddings are extracted using the **ArcFace model**. The cleartext and encrypted results are compared for accuracy, runtime performance, and ciphertext size.\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "### Dataset\n",
    "- **Train Pairs**: 2,200 pairs.\n",
    "- **Test Pairs**: 1,000 pairs.\n",
    "\n",
    "### Sampling\n",
    "- Unique images used:\n",
    "  - **Templates**: 500 sampled images from the train set.\n",
    "  - **Test Samples**: 300 sampled images from the test set.\n",
    "\n",
    "### Embedding Details\n",
    "- **Embedding Extraction**: ArcFace model (`buffalo_l`).\n",
    "- **Embedding Dimensions**: 512 features per image.\n",
    "\n",
    "### Encryption Scheme\n",
    "- **CKKS Parameters**:\n",
    "  - Polynomial modulus degree: 8192.\n",
    "  - Coefficient modulus bit sizes: `[60, 40, 40, 60]`.\n",
    "  - Global scale: \\( 2^{40} \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "### Cleartext Similarity Matrix\n",
    "- **Runtime**: Computation took **1.02 seconds** for a \\( 300 \\times 500 \\) matrix.\n",
    "- **Output Files**:\n",
    "  - **Scores**: Saved in `output_partC/scores.csv`.\n",
    "  - **Top-10 Similarity Indices**: Saved in `output_partC/top10.csv`.\n",
    "\n",
    "### Homomorphic Similarity Matrix\n",
    "- **Runtime**: Computation took **7,800.92 seconds** (~2.17 hours).\n",
    "- **Output Files**:\n",
    "  - **Scores**: Saved in `output_partC/scores_dec.csv`.\n",
    "  - **Top-10 Similarity Indices**: Saved in `output_partC/top10_dec.csv`.\n",
    "\n",
    "### Accuracy Comparison\n",
    "The decrypted results from the homomorphic similarity computation were compared to the cleartext results.\n",
    "\n",
    "- **Absolute Difference**:\n",
    "  - **Average**: \\( 0.000002 \\).\n",
    "  - **Standard Deviation**: \\( 0.000000 \\).\n",
    "  - **Maximum**: \\( 0.000003 \\).\n",
    "  - **Minimum**: \\( 0.000000 \\).\n",
    "\n",
    "- **Top-10 Rank Consistency**:\n",
    "  - **Column-wise Matching**:\n",
    "    - All ranks (0 through 9) matched with **100.00%** accuracy.\n",
    "  - **Exact Top-10 List Matching**:\n",
    "    - **100.00%** of the lists matched exactly for all test samples.\n",
    "\n",
    "### Ciphertext Size\n",
    "- **Average Ciphertext Size**: ~334,330 bytes per embedding.\n",
    "\n",
    "---\n",
    "\n",
    "## Runtime Summary\n",
    "- **Cleartext Similarity**: **1.02 seconds**.\n",
    "- **Homomorphic Similarity**: **7,800.92 seconds** (~2.17 hours).\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusions\n",
    "1. **Accuracy**:\n",
    "   - The homomorphic similarity results matched the cleartext results with minimal differences (absolute difference \\( \\leq 0.000003 \\)).\n",
    "   - Top-10 ranks were consistent across all test samples.\n",
    "2. **Performance**:\n",
    "   - Homomorphic computation is significantly slower than cleartext computation due to encryption overhead.\n",
    "3. **Ciphertext Size**:\n",
    "   - Each encrypted embedding consumes ~334KB, which impacts memory usage and communication overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Directions\n",
    "1. Optimize encryption parameters to reduce runtime and ciphertext size.\n",
    "2. Explore batching strategies for more efficient homomorphic computations.\n",
    "3. Evaluate performance on larger datasets and real-world scenarios.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LFW pairs...\n",
      "Train pairs: 2200, Test pairs: 1000\n",
      "Initializing ArcFace model (buffalo_l)...\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Alpha/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (112, 112)\n",
      "Number of unique template images: 3443\n",
      "Number of unique test-sample images: 1549\n",
      "After sampling: #templates = 500, #samples = 300\n",
      "Building embeddings for templates...\n",
      "Building embeddings for test samples...\n",
      "\n",
      "Computing cleartext similarity matrix [n x m] ...\n",
      "Cleartext similarity computation took 1.02 s\n",
      "Writing output_partC\\scores.csv ...\n",
      "Writing output_partC\\top10.csv ...\n",
      "\n",
      "Initializing CKKS context & encryption keys...\n",
      "Encrypting template embeddings...\n",
      "Encrypting sample embeddings...\n",
      "\n",
      "Computing homomorphic similarity matrix [n x m] ...\n",
      "Encrypted similarity computation took 7800.92 s\n",
      "Writing output_partC\\scores_dec.csv ...\n",
      "Writing output_partC\\top10_dec.csv ...\n",
      "\n",
      "Comparing cleartext vs. decrypted similarity matrices...\n",
      "Absolute difference: avg=0.000002, std=0.000000, max=0.000003, min=0.000000\n",
      "Top-10 rank consistency (by column):\n",
      "  Rank 0 match = 100.00%\n",
      "  Rank 1 match = 100.00%\n",
      "  Rank 2 match = 100.00%\n",
      "  Rank 3 match = 100.00%\n",
      "  Rank 4 match = 100.00%\n",
      "  Rank 5 match = 100.00%\n",
      "  Rank 6 match = 100.00%\n",
      "  Rank 7 match = 100.00%\n",
      "  Rank 8 match = 100.00%\n",
      "  Rank 9 match = 100.00%\n",
      "Exact top-10 list match for all columns: 100.00%\n",
      "\n",
      "=== Runtime Summary ===\n",
      "Cleartext similarity took: 1.02 s\n",
      "Homomorphic similarity took: 7800.92 s\n",
      "\n",
      "Ciphertext size example (~one embedding): ~334330.0 bytes\n",
      "\n",
      "Part C script completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import statistics\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import tenseal as ts\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "BASE_DIR = os.path.join(\"dataset\", \"lfw-deepfunneled\", \"lfw-deepfunneled\")\n",
    "PAIRS_TRAIN_PATH = \"pairsDevTrain.txt\"\n",
    "PAIRS_TEST_PATH = \"pairsDevTest.txt\"\n",
    "\n",
    "EMBED_DIM = 512\n",
    "\n",
    "OUTPUT_DIR = \"output_partC\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def normalize_name(name):\n",
    "    return name.replace(\" \", \"_\")\n",
    "\n",
    "def load_pairs(pairs_path, base_dir):\n",
    "    \"\"\"Load LFW pairs text file -> list of (img1_path, img2_path, label).\"\"\"\n",
    "    pairs = []\n",
    "    with open(pairs_path, \"r\") as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                person, img1, img2 = parts\n",
    "                person = normalize_name(person)\n",
    "                img1_path = os.path.join(base_dir, person, f\"{person}_{int(img1):04d}.jpg\")\n",
    "                img2_path = os.path.join(base_dir, person, f\"{person}_{int(img2):04d}.jpg\")\n",
    "                if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                    pairs.append((img1_path, img2_path, 1))\n",
    "            elif len(parts) == 4:\n",
    "                p1, img1, p2, img2 = parts\n",
    "                p1, p2 = normalize_name(p1), normalize_name(p2)\n",
    "                img1_path = os.path.join(base_dir, p1, f\"{p1}_{int(img1):04d}.jpg\")\n",
    "                img2_path = os.path.join(base_dir, p2, f\"{p2}_{int(img2):04d}.jpg\")\n",
    "                if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                    pairs.append((img1_path, img2_path, 0))\n",
    "    return pairs\n",
    "\n",
    "print(\"Loading LFW pairs...\")\n",
    "train_pairs = load_pairs(PAIRS_TRAIN_PATH, BASE_DIR)\n",
    "test_pairs = load_pairs(PAIRS_TEST_PATH, BASE_DIR)\n",
    "all_pairs = train_pairs + test_pairs\n",
    "print(f\"Train pairs: {len(train_pairs)}, Test pairs: {len(test_pairs)}\")\n",
    "\n",
    "print(\"Initializing ArcFace model (buffalo_l)...\")\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=[\"CUDAExecutionProvider\",\"CPUExecutionProvider\"])\n",
    "app.prepare(ctx_id=0, det_size=(112, 112))\n",
    "\n",
    "def get_arcface_embedding(img_path):\n",
    "    bgr_img = cv2.imread(img_path)\n",
    "    if bgr_img is None:\n",
    "        raise ValueError(f\"Could not load image {img_path}\")\n",
    "    bgr_img = cv2.resize(bgr_img, (112, 112))\n",
    "    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "    feat = app.models[\"recognition\"].get_feat(rgb_img)\n",
    "\n",
    "    if feat.ndim == 2:\n",
    "        feat = feat[0]\n",
    "\n",
    "    feat_norm = feat / np.linalg.norm(feat)\n",
    "    return feat_norm.astype(np.float32)\n",
    "\n",
    "def build_embeddings_dict(image_paths):\n",
    "    \"\"\"Compute and cache embeddings for each unique image path.\"\"\"\n",
    "    emb_dict = {}\n",
    "    for path in image_paths:\n",
    "        emb_dict[path] = get_arcface_embedding(path)\n",
    "    return emb_dict\n",
    "\n",
    "template_paths = set()\n",
    "test_sample_paths = set()\n",
    "for (img1, img2, label) in train_pairs:\n",
    "    template_paths.add(img1)\n",
    "    template_paths.add(img2)\n",
    "for (img1, img2, label) in test_pairs:\n",
    "    test_sample_paths.add(img1)\n",
    "    test_sample_paths.add(img2)\n",
    "\n",
    "template_paths = list(template_paths)\n",
    "test_sample_paths = list(test_sample_paths)\n",
    "\n",
    "print(f\"Number of unique template images: {len(template_paths)}\")\n",
    "print(f\"Number of unique test-sample images: {len(test_sample_paths)}\")\n",
    "\n",
    "random.shuffle(template_paths)\n",
    "random.shuffle(test_sample_paths)\n",
    "\n",
    "MAX_TEMPLATES = 500\n",
    "MAX_SAMPLES   = 300\n",
    "\n",
    "template_paths = template_paths[:MAX_TEMPLATES]\n",
    "test_sample_paths = test_sample_paths[:MAX_SAMPLES]\n",
    "\n",
    "print(f\"After sampling: #templates = {len(template_paths)}, #samples = {len(test_sample_paths)}\")\n",
    "\n",
    "print(\"Building embeddings for templates...\")\n",
    "template_emb_dict = build_embeddings_dict(template_paths)\n",
    "\n",
    "print(\"Building embeddings for test samples...\")\n",
    "sample_emb_dict = build_embeddings_dict(test_sample_paths)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Cosine similarity for 1D arrays.\"\"\"\n",
    "    vec1 = vec1.ravel()\n",
    "    vec2 = vec2.ravel()\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    return dot / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "sample_list = test_sample_paths\n",
    "template_list = template_paths\n",
    "\n",
    "n = len(sample_list)\n",
    "m = len(template_list)\n",
    "\n",
    "scores_matrix = np.zeros((n, m), dtype=np.float32)\n",
    "\n",
    "print(\"\\nComputing cleartext similarity matrix [n x m] ...\")\n",
    "t0 = time.time()\n",
    "for i, sample_path in enumerate(sample_list):\n",
    "    emb_s = sample_emb_dict[sample_path]\n",
    "    for j, template_path in enumerate(template_list):\n",
    "        emb_t = template_emb_dict[template_path]\n",
    "        sim = cosine_similarity(emb_s, emb_t)\n",
    "        scores_matrix[i, j] = sim\n",
    "cleartext_time = time.time() - t0\n",
    "print(f\"Cleartext similarity computation took {cleartext_time:.2f} s\")\n",
    "\n",
    "scores_csv_path = os.path.join(OUTPUT_DIR, \"scores.csv\")\n",
    "print(f\"Writing {scores_csv_path} ...\")\n",
    "np.savetxt(scores_csv_path, scores_matrix, delimiter=\",\", fmt=\"%.5f\")\n",
    "\n",
    "top10_indices = []\n",
    "for i in range(n):\n",
    "    row = scores_matrix[i, :]\n",
    "    top10 = np.argsort(-row)[:10]\n",
    "    top10_indices.append(top10)\n",
    "\n",
    "top10_csv_path = os.path.join(OUTPUT_DIR, \"top10.csv\")\n",
    "print(f\"Writing {top10_csv_path} ...\")\n",
    "with open(top10_csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(n):\n",
    "        writer.writerow(top10_indices[i].tolist())\n",
    "\n",
    "print(\"\\nInitializing CKKS context & encryption keys...\")\n",
    "\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=8192,\n",
    "    coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
    ")\n",
    "context.global_scale = 2 ** 40\n",
    "context.generate_galois_keys()\n",
    "\n",
    "def encrypt_vector(vec):\n",
    "    return ts.ckks_vector(context, vec)\n",
    "\n",
    "enc_template_emb_dict = {}\n",
    "enc_sample_emb_dict = {}\n",
    "\n",
    "print(\"Encrypting template embeddings...\")\n",
    "for path in template_list:\n",
    "    enc_template_emb_dict[path] = encrypt_vector(template_emb_dict[path])\n",
    "\n",
    "print(\"Encrypting sample embeddings...\")\n",
    "for path in sample_list:\n",
    "    enc_sample_emb_dict[path] = encrypt_vector(sample_emb_dict[path])\n",
    "\n",
    "def homomorphic_cosine_similarity(enc_vec1, enc_vec2):\n",
    "    dot_product = (enc_vec1 * enc_vec2).sum()\n",
    "    norm1 = (enc_vec1 * enc_vec1).sum()\n",
    "    norm2 = (enc_vec2 * enc_vec2).sum()\n",
    "\n",
    "    decrypted_dot = dot_product.decrypt()[0]\n",
    "    decrypted_norm1 = norm1.decrypt()[0]\n",
    "    decrypted_norm2 = norm2.decrypt()[0]\n",
    "    return decrypted_dot / (np.sqrt(decrypted_norm1) * np.sqrt(decrypted_norm2))\n",
    "\n",
    "scores_enc_matrix = np.zeros((n, m), dtype=np.float32)\n",
    "\n",
    "print(\"\\nComputing homomorphic similarity matrix [n x m] ...\")\n",
    "t0 = time.time()\n",
    "for i, sample_path in enumerate(sample_list):\n",
    "    enc_s = enc_sample_emb_dict[sample_path]\n",
    "    for j, template_path in enumerate(template_list):\n",
    "        enc_t = enc_template_emb_dict[template_path]\n",
    "        sim_enc = homomorphic_cosine_similarity(enc_s, enc_t)\n",
    "        scores_enc_matrix[i, j] = sim_enc\n",
    "homomorphic_time = time.time() - t0\n",
    "print(f\"Encrypted similarity computation took {homomorphic_time:.2f} s\")\n",
    "\n",
    "scores_dec_csv_path = os.path.join(OUTPUT_DIR, \"scores_dec.csv\")\n",
    "print(f\"Writing {scores_dec_csv_path} ...\")\n",
    "np.savetxt(scores_dec_csv_path, scores_enc_matrix, delimiter=\",\", fmt=\"%.5f\")\n",
    "\n",
    "top10_dec_indices = []\n",
    "for i in range(n):\n",
    "    row = scores_enc_matrix[i, :]\n",
    "    top10 = np.argsort(-row)[:10]\n",
    "    top10_dec_indices.append(top10)\n",
    "\n",
    "top10_dec_csv_path = os.path.join(OUTPUT_DIR, \"top10_dec.csv\")\n",
    "print(f\"Writing {top10_dec_csv_path} ...\")\n",
    "with open(top10_dec_csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(n):\n",
    "        writer.writerow(top10_dec_indices[i].tolist())\n",
    "\n",
    "print(\"\\nComparing cleartext vs. decrypted similarity matrices...\")\n",
    "\n",
    "cleartext_scores = scores_matrix\n",
    "dec_scores = scores_enc_matrix\n",
    "\n",
    "diff_matrix = np.abs(cleartext_scores - dec_scores)\n",
    "avg_diff = diff_matrix.mean()\n",
    "std_diff = diff_matrix.std()\n",
    "max_diff = diff_matrix.max()\n",
    "min_diff = diff_matrix.min()\n",
    "\n",
    "print(f\"Absolute difference: avg={avg_diff:.6f}, std={std_diff:.6f}, \"\n",
    "      f\"max={max_diff:.6f}, min={min_diff:.6f}\")\n",
    "\n",
    "top10_clear = []\n",
    "with open(top10_csv_path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        top10_clear.append(list(map(int, row)))\n",
    "\n",
    "top10_dec = []\n",
    "with open(top10_dec_csv_path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        top10_dec.append(list(map(int, row)))\n",
    "\n",
    "match_percentages = []\n",
    "for col_idx in range(10):\n",
    "    match_count = 0\n",
    "    for sample_idx in range(n):\n",
    "        if top10_clear[sample_idx][col_idx] == top10_dec[sample_idx][col_idx]:\n",
    "            match_count += 1\n",
    "    match_percentages.append(100.0 * match_count / n)\n",
    "\n",
    "print(\"Top-10 rank consistency (by column):\")\n",
    "for i, pct in enumerate(match_percentages):\n",
    "    print(f\"  Rank {i} match = {pct:.2f}%\")\n",
    "\n",
    "exact_top10_match_count = 0\n",
    "for sample_idx in range(n):\n",
    "    if top10_clear[sample_idx] == top10_dec[sample_idx]:\n",
    "        exact_top10_match_count += 1\n",
    "exact_top10_pct = 100.0 * exact_top10_match_count / n\n",
    "print(f\"Exact top-10 list match for all columns: {exact_top10_pct:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Runtime Summary ===\")\n",
    "print(f\"Cleartext similarity took: {cleartext_time:.2f} s\")\n",
    "print(f\"Homomorphic similarity took: {homomorphic_time:.2f} s\")\n",
    "\n",
    "enc_size_samples = []\n",
    "for path in sample_list[:5]:\n",
    "    serialized = enc_sample_emb_dict[path].serialize()\n",
    "    enc_size_samples.append(len(serialized))\n",
    "\n",
    "avg_ciphertext_size = sum(enc_size_samples)/len(enc_size_samples)\n",
    "print(f\"\\nCiphertext size example (~one embedding): ~{avg_ciphertext_size} bytes\")\n",
    "\n",
    "print(\"\\nPart C script completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
